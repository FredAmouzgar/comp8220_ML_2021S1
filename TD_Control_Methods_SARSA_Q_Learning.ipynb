{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD_Control_Methods-SARSA_Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Njd49MQURU"
      },
      "source": [
        "# 1. Sarsa Algorithm and the OpenAI Gym's Taxi Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iyniLZnQqEr"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Tabular methods are suitable for small and discrete state space and discrete action space environments. So, the state-action function (Q) can be represented by a table of values. For large state space environments, we prefer to use approximation methods such as neural networks. However, the simplicity of tabular methods' implementation is helpful to demonstrate RL method's functionality. In this notebook, we train a SARSA agent for OpenAI's Taxi Gym environment ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCJyC-NeaJh-"
      },
      "source": [
        "## Goal\n",
        "\n",
        "In this notebook, we are going to tackle the <a href=\"https://gym.openai.com/envs/Taxi-v2/\">Taxi environment</a> (an example originally proposed by Tom Dietterich) with the Sarsa and Q-Learning algorithms. Once we implement Sarsa, implementing Q-Learning and many others would be easy. We usually need to change the core update method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKMCFuJ_RyM8"
      },
      "source": [
        "## The Taxi Environment\n",
        "\n",
        "Here is a description of the taxi environment from the [docstring](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py).\n",
        "\n",
        "The Smartcab's job is to pick up the passenger in a simplified gridworld like environment at one location and drop them off in another ([Taxi, OpenAI Gym](https://gym.openai.com/envs/Taxi-v2/)). Here are a few things that we'd love our Smartcab to take care of:\n",
        "\n",
        "- Drop off the passenger to the right location.\n",
        "- Save passenger's time by taking minimum time possible to drop off\n",
        "- Take care of passenger's safety and traffic rules\n",
        "\n",
        "<center><img style=\"align: center;\" src=\"https://github.com/FredAmouzgar/comp8220_ML_2021S1/raw/main/images/Taxi_Env.png\" width=400></center>\n",
        "\n",
        "__State__: Let's say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):\n",
        "\n",
        "Let's assume Smartcab is the only vehicle in this parking lot. We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. Notice the current location state of our taxi is coordinate (3, 1).\n",
        "\n",
        "You'll also notice there are four (4) locations that we can pick up and drop off a passenger: R, G, Y, B or `[(0,0), (0,4), (4,0), (4,3)]` in (row, col) coordinates. Our illustrated passenger is in location Y and they wish to go to location R.\n",
        "\n",
        "__Actions__: The agent is allowed to perform six possible actions:\n",
        "\n",
        "1. south\n",
        "2. north\n",
        "3. east\n",
        "4. west\n",
        "5. pickup\n",
        "6. dropoff\n",
        "\n",
        "Notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment's code, we will simply provide a -1 penalty for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfmOQaceaXW0"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO6-LNuhabtM"
      },
      "source": [
        "!pip -q install gym numpy matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7POfWCiuamR-"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwTm5qKhapTl",
        "outputId": "90412677-0173-43fb-f053-719793021f47"
      },
      "source": [
        "import gym\n",
        "\n",
        "# Finding the Taxi environment\n",
        "for env in gym.envs.registry.all():\n",
        "    if env.id.startswith(\"Taxi\"):\n",
        "        env_name = env.id\n",
        "##\n",
        "\n",
        "\n",
        "print(\"Environment Name:\", env_name)\n",
        "env = gym.make(env_name)\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment Name: Taxi-v3\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY6G9AoUTGA7"
      },
      "source": [
        "## The SARSA Algorithm\n",
        "<p> </p><br>\n",
        "<center><img style=\"align: center;\" src=\"https://github.com/FredAmouzgar/comp8220_ML_2021S1/raw/main/images/SARSA_algorithm.png\" width=900></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5p2m8SvQjXg"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sarsa_Agent:\n",
        "    def __init__(self, states_n, actions_n, alpha=0.1, epsilon=0.1, gamma=0.95, epsilon_decay=True,\n",
        "                 epsilon_decay_factor=0.01):\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.states_n = states_n\n",
        "        self.actions_n = actions_n\n",
        "        self.Q = np.zeros((states_n, actions_n))\n",
        "        self.new_a = None\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_decay_factor = epsilon_decay_factor\n",
        "\n",
        "    def act(self, state):\n",
        "        # epsilon greedy\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            act = np.random.choice(np.arange(self.actions_n))\n",
        "        else:\n",
        "            act = np.argmax(self.Q[int(state), :])\n",
        "        return act\n",
        "\n",
        "    def decay_epsilon(self, factor):\n",
        "        self.epsilon -= factor if self.epsilon >= 0 else 0\n",
        "\n",
        "    def update(self, new_s, r, s, a, done):\n",
        "        self.new_a = self.act(new_s)\n",
        "        mask = 0 if done else 1\n",
        "        s, a, self.new_a, new_s = int(s), int(a), int(self.new_a), int(new_s)\n",
        "        self.Q[s, a] += self.alpha * (r + self.gamma * self.Q[new_s, self.new_a] * mask - self.Q[s, a])\n",
        "        if done and self.epsilon_decay:\n",
        "            self.decay_epsilon(self.epsilon_decay_factor)\n",
        "        return self.new_a"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfARjOAdkBy"
      },
      "source": [
        "## The Train Loop\n",
        "<p> </p><br>\n",
        "<center><img style=\"align: center;\" src=\"https://github.com/FredAmouzgar/comp8220_ML_2021S1/raw/main/images/MDP_loop.jpeg\" width=900></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMDBL2lpRHCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xdg580We0cq"
      },
      "source": [
        "# How the learning happened?\n",
        "\n",
        "The Sarsa agent is implementing a __tabular method__ which means that it's updating a table of values iteratively. This table is called Q-table. After the training, the agent knows exactly which actions are the best in almost all possible states.\n",
        "\n",
        "In __Approximation methods__ and __Deep Reinforcement Learning__, this table is replaced by a linear model or a [neural network](https://www.nature.com/articles/nature14236). A well-designed network can learn much more complex tasks, generalizes, and thus responds better to unseen situations.\n",
        "\n",
        "__Take a look at the Pigoen Pong Experiment__ (special thanks go to <a href=\"https://complexity-in-action.github.io/people/patricknalepka.html\">Dr. Patrick Nalepka</a>): Artificial agents follow the same process. __We'll show an agent with neural network on week 12.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "9l-QACZIeY5R",
        "outputId": "643ba500-856c-441e-f9e7-b76c5d059e40"
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/vGazyH6fQQ4\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/vGazyH6fQQ4\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViTFBApneZrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGoPr0V0eb1i"
      },
      "source": [
        "<font size=\"1px\">A modified version of our <a href=\"https://github.com/probml/pyprobml/blob/master/book2/supplements/rl/Tabular_SARSA.ipynb\">joint work</a> with Prof. Kevin Murphy for the second edition of his book, <a href=\"https://probml.github.io/pml-book/\">Machine Learning: A Probabilistic Perspective</a></font>\n",
        "\n",
        "<font size=\"1px\"><a href=\"https://www.linkedin.com/in/fredamouzgar/\">Fred A.</a> Mar/2021</font>"
      ]
    }
  ]
}